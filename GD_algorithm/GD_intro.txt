Gradient descent algorithm in CNN

There are so many optimization problems in statistics, data science, and machine learning. Such as:Linear regression, optimize the intercept and slope. Logistic regression, optimize the squiggle. t-SNE, optimize clusters.

Gradient descent(GD) can be used in all the situations above as well as neural networks. By adjusting its parameters, it can be used to optimize complex functions and models.

What is GD?

Gradient descent is a one-order iterative optimization algorithm used to find the local minima of a given function.It works by taking small steps in the direction of negative gradient until it reaches a point where the gradient is zero or close to zero.

The step size (learning rate) determines how quickly the algorithm converges to a solution, but too large of a step size can cause it to diverge instead of converging.

Stochastic gradient descent is an alternative version which uses randomly selected samples from the data set instead of using all data points at each iteration, allowing for faster convergence on large datasets with many features.

It can be used for both linear and non-linear problems in machine learning, neural networks, support vector machines(SVM).

Please find more details in GD_example.ipynb

useful links: https://www.youtube.com/watch?v=sDv4f4s2SB8

